# adversarial-attack-filter
초해상화 모델에 적대적 학습을 수행하여 AI 모델에 대한 적대적 공격을 상쇄하는 필터를 개발한 프로젝트입니다. 


## 목차
* [프로젝트 설명](#프로젝트-설명)
* [모델 구조](#모델-구조)
* 데이터셋
* 학습 방법
* 결과
* 개발환경
* 코드
* 참고자료

## 프로젝트 설명
<b>적대적 공격</b>이란, 의도적으로 이미지를 정교하게 조작하여 AI 분류 모델의 오동작을 유발하는 공격입니다. (ex)판다를 책상으로 분류) <br>
적대적 학습이나, 전처리를 통한 방어와 같이 적대적 공격에 대한 방어 방법들이 제시되었지만, 기존 방법들은 다음과 같은 문제가 있습니다. 
 
 * 적대적 학습<br>
 AI 모델에 적대적 공격을 가한 이미지를 추가로 학습시키는 방법입니다. <br>
 그러나 적대적 학습은 각 <u>모델마다 별도로 추가 학습</u>이 필요하다는 단점이 있습니다.. <br>
 * 전처리를 통한 방어<br>
 Randomization이나 초해상화를 통해 이미지를 전처리함으로써, 공격 상쇄 효과가 있다는 것이 확인되었습니다. <br>
 그러나 고도화된 적대적 공격에 대해서는 낮은 방어율을 보입니다.  

본 프로젝트에서는 두 방식의 장점을 결합하고 단점을 상쇄하도록 초해상화 모델에 적대적 학습을 수행하여, <b>적대적 공격을 상쇄해주는 필터</b>를 개발하였습니다.  
이러한 방식을 통해 다음과 같은 장점을 얻을 수 있습니다.

 * 사용 중이던 AI 모델을 별도로 학습할 필요가 없습니다.
 * 기존 전처리 방식보다 더 높은 방어율을 보입니다. 
 
## 모델 구조
본 프로젝트에서 개발한 적대적 필터는 다음과 같은 구조를 갖습니다.  

①입력 이미지에 대한 다운샘플링을 수행하고,  
②다운샘플된 이미지를 적대적 학습된 초해상화 모델로 다시 복원합니다. 
<p align = "center">
<img src = "https://user-images.githubusercontent.com/71579787/209918632-f8729954-6549-4dc6-9a29-dc5f005c8398.png" width = "750" ></p>

각 단계는 다음과 같은 역할을 수행합니다.
 * 다운샘플링: 다운샘플링을 통한 적대적 공격 상쇄 효과가 있습니다. 다만 이미지 자체에도 손상이 발생합니다.
 * 초해상화: 다운샘플링 과정에서 손실된 정보를 복원하고, 적대적 공격을 추가로 상쇄합니다. 




 
 
 
 
 
